{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Zadanie 2.\n",
    "\n",
    "Przygotuj chmurę słów (lub wykres kolumnowy dla 50 najczęściej używanych słów) w oparciu o *bag-of-words*. Dla pozyskanych przez Ciebie dokumentów (w języku angielskim). Korpus ma odzwierciedlać pewną dychotomię - konflikt, dyskusję dwóch stron pewnego zjawiska np: zmian klimatycznych, lockdownu, ulubionego sportu (rugby vs. piłka nożna). Mogą to być: teksty tweetów na wybrane tematy (min 1000 tweetów dla każdej strony); dwóch grup artykułów (po 3-5 dla każdej strony, każdy ponad 2000 wyrazów); wypowiedzi dwóch ekspertów czy polityków (po 3-5 dla każdej strony, każdy ponad 2000 wyrazów).\n",
    "\n",
    "Analizę wykonaj w dwóch wersjach:\n",
    "\n",
    "1. dwa teksty traktowane są osobno - jako dwa osobne korpusy, przygotuj dwie wizualizacje dla każdej strony osobno.\n",
    "2. dwa teksty traktowane są jako jeden i wykonaj wizualizację prezentującą:\n",
    "\n",
    "    - termów charakterystycznych dla każdej ze stron (termy pojawiające się w wypowiedziach jednej strony ale nie pojawiające się w wypowiedziach drugiej strony),\n",
    "    - termów wspólnych dla dwóch stron (termy pojawiające się w wypowiedziach jednej i drugiej strony jednocześnie).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "import nest_asyncio\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('SnowballStemmer')\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = twint.Config()\n",
    "# c.Search = '#verstappen'\n",
    "# c.Limit = 5\n",
    "# twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja pobierająca dane za pomocą biblioteki twint\n",
    "def get_data(query, limit):\n",
    "    c = twint.Config()\n",
    "    c.Search = query\n",
    "    c.Lang = 'en'\n",
    "    c.Limit = limit\n",
    "    c.Pandas = True\n",
    "    twint.run.Search(c)\n",
    "    tweets = twint.storage.panda.Tweets_df\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcja oczyszczająca tweety\n",
    "def cleaned_tweets(text):\n",
    "    # normalizacja tekstu\n",
    "    temp = re.sub(\"\\s{2,}\", \" \", text)\n",
    "    temp = re.sub(\"(\\r\\n|\\r|\\n)\", \" \", temp) \n",
    "    temp = temp.lower() \n",
    "    temp = re.sub(\"rt\", \"\", temp) \n",
    "    temp = re.sub(\"&amp\", \"\", temp) \n",
    "    temp = re.sub(\"#[a-z,A-Z]*\", \"\", temp)\n",
    "    temp = re.sub(\"@\\w+\", \"\", temp) \n",
    "    temp = re.sub(\"(f|ht)(tp)([^ ]*)\", \"\", temp) \n",
    "    temp = re.sub(\"http(s?)([^ ]*)\", \"\", temp)\n",
    "    temp = re.sub(\"[!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\", \" \", temp) \n",
    "    temp = re.sub(\"\\d\", \"\", temp) \n",
    "    temp = re.sub(\"\\s{2,}\", \" \", temp) \n",
    "    temp = temp.strip()\n",
    "    \n",
    "    # usuwanie duplikatów ze zbioru\n",
    "    words_set = set(temp.split())\n",
    "    words_list = list(words_set)\n",
    "    \n",
    "    # tokenizacja\n",
    "    tokens = nltk.word_tokenize(\" \".join(words_list))\n",
    "    \n",
    "    # tworzenie listy stop słów\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # stemming\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if not token in stop_words]\n",
    "    \n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicja funkcji generującej chmurę słów\n",
    "def generate_wordcloud(data):\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=50, contour_width=3, contour_color='steelblue')\n",
    "    wordcloud.generate(data)\n",
    "    plt.figure(figsize = (8, 8), facecolor = 'k', edgecolor = 'k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobranie danych z Twittera, 50 tweetów\n",
    "#data1 = get_data(\"#climatechange -filter:retweets\", 50)\n",
    "#data2 = get_data(\"#globalwarming -filter:retweets\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oczyszczenie tweetów\n",
    "#clean_data1 = [cleaned_tweets(tweet) for tweet in data1['tweet']]\n",
    "#clean_data2 = [cleaned_tweets(tweet) for tweet in data2['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Połączenie tweetów w jedną listę\n",
    "#all_data = clean_data1 + clean_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pobranie danych dla obu grup tweetów\n",
    "group1_tweets = get_data('climate change', 1000)\n",
    "group2_tweets = get_data('global warming', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oczyszczenie tweetów\n",
    "group1_cleaned = [cleaned_tweets(tweet) for tweet in group1_tweets['tweet']]\n",
    "group2_cleaned = [cleaned_tweets(tweet) for tweet in group2_tweets['tweet']]\n",
    "\n",
    "group1_words = set([word for tweet in group1_cleaned for word in tweet])\n",
    "group2_words = set([word for tweet in group2_cleaned for word in tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wygenerowanie chmury słów\n",
    "generate_wordcloud(' '.join(group1_words))\n",
    "generate_wordcloud(' '.join(group2_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Przygotowanie chmury słów na podstawie bag-of-words dla dwóch tekstów traktowanych jako jeden\n",
    "group1_articles = get_data('climate change', 1000)\n",
    "group2_articles = get_data('global warming', 1000)\n",
    "\n",
    "corpus = group1_articles + group2_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oczyszczenie tweetów z całego korpusu\n",
    "corpus_cleaned = [cleaned_tweets(article) for article in corpus]\n",
    "\n",
    "group1_words = set([word for article in corpus_cleaned[:5] for word in article])\n",
    "group2_words = set([word for article in corpus_cleaned[5:] for word in article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_group1_words = group1_words - group2_words\n",
    "unique_group2_words = group2_words - group1_words\n",
    "common_words = group1_words.intersection(group2_words)\n",
    "\n",
    "generate_wordcloud(' '.join(unique_group1_words))\n",
    "generate_wordcloud(' '.join(unique_group2_words))\n",
    "generate_wordcloud(' '.join(common_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
